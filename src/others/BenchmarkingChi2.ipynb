{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import chi2 as chi2_sklearn\n",
    "from chi2 import chi2_numba\n",
    "\n",
    "# Set up the test parameters\n",
    "N_SAMPLES = 2000\n",
    "N_FEATURES = 2000\n",
    "N_CLASSES = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Chi-Squared Implementation Benchmark\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Dataset shape: Samples={N_SAMPLES}, Features={N_FEATURES}, Classes={N_CLASSES}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=N_SAMPLES,\n",
    "    n_features=N_FEATURES,\n",
    "    n_informative=500,\n",
    "    n_redundant=500,\n",
    "    n_classes=N_CLASSES,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "# The Chi-squared test requires non-negative features (e.g., counts)\n",
    "X = np.abs(X * 100).astype(np.int64)\n",
    "\n",
    "# 2. Run the Numba implementation\n",
    "# First run is for JIT compilation (\"warm-up\") and is not timed.\n",
    "print(\"Compiling Numba function...\")\n",
    "chi2_numba(X, y)\n",
    "print(\"Compilation complete.\\n\")\n",
    "\n",
    "# Time the Numba implementation\n",
    "print(\"Timing Numba implementation...\")\n",
    "numba_time = timeit.timeit(lambda: chi2_numba(X, y), number=10)\n",
    "print(f\"Done.\")\n",
    "\n",
    "# 3. Run the scikit-learn implementation\n",
    "print(\"\\nTiming scikit-learn implementation...\")\n",
    "sklearn_time = timeit.timeit(lambda: chi2_sklearn(X, y), number=10)\n",
    "print(f\"Done.\")\n",
    "\n",
    "# 4. Verify that the results are the same\n",
    "chi2_n, p_n = chi2_numba(X, y)\n",
    "chi2_s, p_s = chi2_sklearn(X, y)\n",
    "\n",
    "assert np.allclose(chi2_n, chi2_s), \"Chi2 statistics do not match!\"\n",
    "assert np.allclose(p_n, p_s), \"P-values do not match!\"\n",
    "print(\"\\nCorrectness check passed: Results are identical.\")\n",
    "\n",
    "# 5. Report the results\n",
    "print(\"\\n\\n--- Benchmark Results ---\")\n",
    "print(f\"Scikit-learn time: {sklearn_time:.4f} seconds\")\n",
    "print(f\"Numba time:        {numba_time:.4f} seconds\")\n",
    "\n",
    "speedup = sklearn_time / numba_time\n",
    "print(f\"\\nNumba implementation is {speedup:.2f}x faster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4702f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galynch/.local/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Warming up Numba JIT compilers ---\n",
      "done\n",
      "--- Warm-up complete. ---\n",
      "\n",
      "================================================================================\n",
      "RUNNING: 1000 samples, 200 features, selecting 100\n",
      "================================================================================\n",
      "--- Timing Execution ---\n",
      "done\n",
      "Your mRMR (Numba)           : 0.1931 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 28.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrmr-selection (C++)        : 4.7276 s\n",
      "Feature-engine (MID)        : 35.5496 s\n",
      "\n",
      "--- Verifying Feature Set Similarity (Jaccard Index) ---\n",
      "                      Your mRMR (Numba)  mrmr-selection (C++)  Feature-engine (MID)\n",
      "Your mRMR (Numba)                 1.000                 0.626                 0.639\n",
      "mrmr-selection (C++)              0.626                 1.000                 0.613\n",
      "Feature-engine (MID)              0.639                 0.613                 1.000\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RUNNING: 200 samples, 2000 features, selecting 100\n",
      "================================================================================\n",
      "--- Timing Execution ---\n",
      "done\n",
      "Your mRMR (Numba)           : 1.3164 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrmr-selection (C++)        : 7.1597 s\n",
      "Feature-engine (MID)        : 134.0680 s\n",
      "\n",
      "--- Verifying Feature Set Similarity (Jaccard Index) ---\n",
      "                      Your mRMR (Numba)  mrmr-selection (C++)  Feature-engine (MID)\n",
      "Your mRMR (Numba)                 1.000                 0.361                 0.639\n",
      "mrmr-selection (C++)              0.361                 1.000                 0.325\n",
      "Feature-engine (MID)              0.639                 0.325                 1.000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final_benchmark_with_feature_engine.py\n",
    "import os\n",
    "os.environ['NUMBA_NUM_THREADS'] = '8'\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit, prange\n",
    "from math import log\n",
    "from sklearn.feature_selection import chi2, f_regression, mutual_info_classif, mutual_info_regression\n",
    "from itertools import combinations\n",
    "from numba.types import Tuple\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange, float32, int32, int64\n",
    "\n",
    "# Numba type signatures for eager compilation\n",
    "# This signature assumes X is float32 and y is int32. Adjust if different.\n",
    "# It reads as: \"A function that returns a float32, and takes two float32 arrays,\n",
    "# their respective number of states (int32), and the number of samples (int64).\"\n",
    "MI_SIGNATURE = float32(float32[:], int32, float32[:], int32, int64)\n",
    "\n",
    "@njit(MI_SIGNATURE, cache=True, nogil=True)\n",
    "def _calculate_mi_optimized(x1, n_states1, x2, n_states2, n_samples):\n",
    "    \"\"\"\n",
    "    Calculates Mutual Information between two discrete vectors.\n",
    "    Uses float32 for better performance and memory usage.\n",
    "    \"\"\"\n",
    "    contingency_table = np.zeros((n_states1, n_states2), dtype=np.float32)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        contingency_table[int(x1[i]), int(x2[i])] += 1\n",
    "\n",
    "    contingency_table /= n_samples\n",
    "\n",
    "    p1 = np.sum(contingency_table, axis=1)\n",
    "    p2 = np.sum(contingency_table, axis=0)\n",
    "    mi = 0.0\n",
    "\n",
    "    for i in range(n_states1):\n",
    "        for j in range(n_states2):\n",
    "            p_xy = contingency_table[i, j]\n",
    "            p_x = p1[i]\n",
    "            p_y = p2[j]\n",
    "            if p_xy > 1e-12 and p_x > 1e-12 and p_y > 1e-12:\n",
    "                mi += p_xy * np.log(p_xy / (p_x * p_y))\n",
    "\n",
    "    return mi\n",
    "\n",
    "# Note the explicit signature. It defines the return tuple and input types.\n",
    "# It reads as: \"A function returning a tuple of (float32[:], float32[:, ::1])\n",
    "# that takes a 2D float32 array (C-contiguous) and a 1D int32 array.\"\n",
    "PRECOMPUTE_SIGNATURE = Tuple((float32[:], float32[:, ::1]))(float32[:, ::1], int32[:])\n",
    "\n",
    "@njit(PRECOMPUTE_SIGNATURE, parallel=True, cache=True)\n",
    "def _precompute_mi_matrices(X, y):\n",
    "    \"\"\"\n",
    "    Precomputes relevance (MI with target) and redundancy (MI between features)\n",
    "    matrices with optimized data types and function calls.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # --- Setup ---\n",
    "    n_states_X = np.zeros(n_features, dtype=np.int32)\n",
    "    for i in prange(n_features):\n",
    "        n_states_X[i] = int(np.max(X[:, i])) + 1\n",
    "    \n",
    "    n_states_y = int(np.max(y)) + 1\n",
    "    y_f32 = y.astype(np.float32)\n",
    "\n",
    "    # --- Stage 1: Relevance Calculation ---\n",
    "    relevance_scores = np.zeros(n_features, dtype=np.float32)\n",
    "    for i in prange(n_features):\n",
    "        relevance_scores[i] = _calculate_mi_optimized(\n",
    "            X[:, i], n_states_X[i], y_f32, n_states_y, n_samples\n",
    "        )\n",
    "\n",
    "    # --- Stage 2: Redundancy Calculation ---\n",
    "    redundancy_matrix = np.zeros((n_features, n_features), dtype=np.float32)\n",
    "    for i in prange(n_features):\n",
    "        for j in range(i + 1, n_features):\n",
    "            mi = _calculate_mi_optimized(\n",
    "                X[:, i], n_states_X[i], X[:, j], n_states_X[j], n_samples\n",
    "            )\n",
    "            redundancy_matrix[i, j] = mi\n",
    "            redundancy_matrix[j, i] = mi\n",
    "\n",
    "    return relevance_scores, redundancy_matrix\n",
    "\n",
    "def fs_mrmr(X, y, k):\n",
    "    \"\"\"\n",
    "    Selects top k features using mRMR, now generalized for ANY discrete data.\n",
    "    This function handles non-contiguous, negative, or large integer categories\n",
    "    by mapping them to zero-based integers before computation.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Feature data (n_samples, n_features). Can contain any discrete values.\n",
    "        y (np.ndarray): Target labels. Can contain any discrete values.\n",
    "        k (int): The number of top features to select.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A sorted array of the top k feature indices.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    if not (0 < k <= n_features):\n",
    "        raise ValueError(\"k must be a positive integer less than or equal to the number of features.\")\n",
    "\n",
    "    unique_values = np.unique(np.concatenate([X.flatten(), y]))\n",
    "    value_to_int = {value: i for i, value in enumerate(unique_values)}\n",
    "\n",
    "    mapper = np.vectorize(value_to_int.get)\n",
    "    X_encoded = mapper(X)\n",
    "    y_encoded = mapper(y)\n",
    "    \n",
    "    # Optimize data types later, numba does not like when you can pass in many different types\n",
    "    X_for_numba = X_encoded.astype(np.float32)\n",
    "    y_for_numba = y_encoded.astype(np.int32)\n",
    "    \n",
    "    relevance, redundancy = _precompute_mi_matrices(X_for_numba, y_for_numba)\n",
    "    selected_features = []\n",
    "\n",
    "    remaining_mask = np.ones(n_features, dtype=bool)\n",
    "\n",
    "    # Select the first feature: the one with the highest relevance.\n",
    "    first_feature_idx = np.argmax(relevance)\n",
    "    selected_features.append(first_feature_idx)\n",
    "    remaining_mask[first_feature_idx] = False\n",
    "\n",
    "    for _ in range(k - 1):\n",
    "        best_score = -np.inf\n",
    "        best_feature = -1\n",
    "        \n",
    "        # Get the indices of features that are still available.\n",
    "        remaining_indices = np.where(remaining_mask)[0]\n",
    "\n",
    "        for candidate_idx in remaining_indices:\n",
    "            # Relevance term: I(candidate; y)\n",
    "            relevance_score = relevance[candidate_idx]\n",
    "            \n",
    "            # Redundancy term: Σ I(candidate; selected) / |S|\n",
    "            # Look up pre-computed MI values. This is extremely fast.\n",
    "            redundancy_score = np.sum(redundancy[candidate_idx, selected_features])\n",
    "            avg_redundancy = redundancy_score / len(selected_features)\n",
    "            \n",
    "            # mRMR score (MID formulation: Relevance - Redundancy)\n",
    "            mrmr_score = relevance_score - avg_redundancy\n",
    "\n",
    "            if mrmr_score > best_score:\n",
    "                best_score = mrmr_score\n",
    "                best_feature = candidate_idx\n",
    "\n",
    "        # Add the best feature found and update the mask\n",
    "        if best_feature != -1:\n",
    "            selected_features.append(best_feature)\n",
    "            remaining_mask[best_feature] = False\n",
    "    print(\"done\")\n",
    "    return np.sort(np.array(selected_features))\n",
    "\n",
    "@njit(parallel=True, cache=True)\n",
    "def _precompute_redundancy_matrix(X):\n",
    "    n_samples, n_features = X.shape\n",
    "    redundancy_matrix = np.zeros((n_features, n_features), dtype=np.float32)\n",
    "    for i in prange(n_features):\n",
    "        for j in range(i + 1, n_features):\n",
    "            mi = _calculate_mi(X[:, i], X[:, j], n_samples)\n",
    "            redundancy_matrix[i, j] = mi\n",
    "            redundancy_matrix[j, i] = mi\n",
    "    return redundancy_matrix\n",
    "\n",
    "def fs_chi2_rmr(X, y, k):\n",
    "    \"\"\"\n",
    "    Selects top k features using a hybrid Chi2-RMR algorithm, now generalized\n",
    "    for ANY discrete data.\n",
    "    \n",
    "    This function handles non-contiguous, negative, or large integer categories\n",
    "    by mapping them to zero-based integers before computation.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Feature data (n_samples, n_features). Can contain any discrete values.\n",
    "        y (np.ndarray): Target labels. Can contain any discrete values.\n",
    "        k (int): The number of top features to select.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: A sorted array of the top k feature indices.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    if k > n_features:\n",
    "        raise ValueError(\"k cannot be greater than the number of features.\")\n",
    "\n",
    "    all_values = np.concatenate([X.flatten(), y.flatten()])\n",
    "    unique_values = np.unique(all_values)\n",
    "    value_to_int = {value: i for i, value in enumerate(unique_values)}\n",
    "    \n",
    "    dtype = np.min_scalar_type(len(unique_values) - 1)\n",
    "    X_encoded = np.empty(X.shape, dtype=dtype)\n",
    "    for i in range(n_features):\n",
    "        for j in range(n_samples):\n",
    "            X_encoded[j, i] = value_to_int[X[j, i]]\n",
    "\n",
    "    y_encoded = np.empty(y.shape, dtype=dtype)\n",
    "    for i in range(n_samples):\n",
    "        y_encoded[i] = value_to_int[y[i]]\n",
    "\n",
    "    \n",
    "    relevance_scores, _ = chi2(X_encoded, y_encoded)\n",
    "    \n",
    "    redundancy_matrix = _precompute_redundancy_matrix(X_encoded)\n",
    "    \n",
    "    selected_features = []\n",
    "    remaining_features = list(range(n_features))\n",
    "    \n",
    "    first_feature_idx = np.argmax(relevance_scores)\n",
    "    selected_features.append(first_feature_idx)\n",
    "    remaining_features.remove(first_feature_idx)\n",
    "\n",
    "    for _ in range(k - 1):\n",
    "        if not remaining_features: break\n",
    "        redundancy_scores = np.sum(redundancy_matrix[remaining_features][:, selected_features], axis=1)\n",
    "        avg_redundancy = redundancy_scores / len(selected_features)\n",
    "        mrmr_scores = relevance_scores[remaining_features] - avg_redundancy\n",
    "        best_idx_in_remaining = np.argmax(mrmr_scores)\n",
    "        best_feature = remaining_features.pop(best_idx_in_remaining)\n",
    "        selected_features.append(best_feature)\n",
    "        \n",
    "    return np.sort(np.array(selected_features))\n",
    "\n",
    "\n",
    "\n",
    "def generate_synthetic_data(n_samples, n_features, n_informative):\n",
    "    \"\"\"Generates synthetic discrete data for benchmarking.\"\"\"\n",
    "    X = np.random.randint(0, 3, size=(n_samples, n_features)).astype(np.int8)\n",
    "    informative_indices = np.arange(n_informative)\n",
    "    y_raw = np.sum(X[:, informative_indices], axis=1)\n",
    "    y = (y_raw > np.median(y_raw)).astype(np.int8)\n",
    "    if n_features > n_informative: X[:, n_informative] = X[:, 0]\n",
    "    if n_features > n_informative + 1: X[:, n_informative + 1] = (X[:, 1] + np.random.randint(0, 2, n_samples)) % 3\n",
    "    return X, y\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculates the Jaccard Index for two sets.\"\"\"\n",
    "    return len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "def main_benchmark_and_verify():\n",
    "    \"\"\"Main function to run the benchmark and print results.\"\"\"\n",
    "    try:\n",
    "        import mrmr\n",
    "        import pymrmr\n",
    "        from feature_engine.selection import MRMR\n",
    "    except ImportError as e:\n",
    "        print(f\"Error importing a library. Please run 'pip install mrmr-selection pymrmr skfeature-chappers feature_engine'.\\nOriginal error: {e}\")\n",
    "        return\n",
    "\n",
    "    benchmark_params = [\n",
    "        (1000, 200, 100),\n",
    "        (200, 2000, 100), # Uncomment for a more intensive test\n",
    "    ]\n",
    "    \n",
    "    # Warm up Numba\n",
    "    print(\"--- Warming up Numba JIT compilers ---\")\n",
    "    X_warmup, y_warmup = generate_synthetic_data(100, 50, 20)\n",
    "    _ = fs_mrmr(X_warmup, y_warmup, k=10)\n",
    "    #_ = fs_chi2_rmr(X_warmup, y_warmup, k=10)\n",
    "    print(\"--- Warm-up complete. ---\\n\")\n",
    "\n",
    "    for n_samples, n_features, k in benchmark_params:\n",
    "        print(\"=\"*80)\n",
    "        print(f\"RUNNING: {n_samples} samples, {n_features} features, selecting {k}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        X_np, y_np = generate_synthetic_data(n_samples, n_features, n_informative=int(k/2))\n",
    "        X_pd = pd.DataFrame(X_np, columns=[f'f{i}' for i in range(n_features)])\n",
    "        \n",
    "        results_time = {}\n",
    "        selected_features = {}\n",
    "        \n",
    "        print(\"--- Timing Execution ---\")\n",
    "        \n",
    "        def run_and_log(name, func, *args):\n",
    "            start = time.perf_counter()\n",
    "            features = func(*args)\n",
    "            duration = time.perf_counter() - start\n",
    "            results_time[name] = duration\n",
    "            selected_features[name] = features\n",
    "            print(f\"{name:<28}: {duration:.4f} s\")\n",
    "\n",
    "        run_and_log('Your mRMR (Numba)', fs_mrmr, X_np, y_np, k)\n",
    "        #run_and_log('Your Chi2-RMR (Numba)', fs_chi2_rmr, X_np, y_np, k)\n",
    "        \n",
    "        # mrmr-selection\n",
    "        run_and_log('mrmr-selection (C++)', lambda: [int(name[1:]) for name in mrmr.mrmr_classif(X=X_pd, y=y_np, K=k)])\n",
    "        \n",
    "        '''# pymrmr\n",
    "        pymrmr_df = X_pd.copy(); pymrmr_df['target'] = y_np\n",
    "        run_and_log('pymrmr (C)', lambda: [int(name[1:]) for name in pymrmr.mRMR(pymrmr_df, 'MID', k)])\n",
    "'''\n",
    "        \n",
    "        # Feature-engine\n",
    "        def run_feature_engine_mid():\n",
    "            fe_mrmr = MRMR(variables=None, max_features=k, method=\"MID\", discrete_features=True, regression=False)\n",
    "            fe_mrmr.fit(X_pd, y_np)\n",
    "            all_cols = set(range(n_features))\n",
    "            dropped_cols = set(int(name[1:]) for name in fe_mrmr.features_to_drop_)\n",
    "            return list(all_cols - dropped_cols)\n",
    "        run_and_log('Feature-engine (MID)', run_feature_engine_mid)\n",
    "\n",
    "        # --- Verification Step ---\n",
    "        print(\"\\n--- Verifying Feature Set Similarity (Jaccard Index) ---\")\n",
    "        \n",
    "        method_names = list(selected_features.keys())\n",
    "        similarity_matrix = pd.DataFrame(np.eye(len(method_names)), index=method_names, columns=method_names)\n",
    "\n",
    "        for (method1, features1), (method2, features2) in combinations(selected_features.items(), 2):\n",
    "            set1 = set(features1); set2 = set(features2)\n",
    "            similarity = jaccard_similarity(set1, set2)\n",
    "            similarity_matrix.loc[method1, method2] = similarity\n",
    "            similarity_matrix.loc[method2, method1] = similarity\n",
    "            \n",
    "        print(similarity_matrix.to_string(float_format=\"%.3f\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_benchmark_and_verify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a603ba54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main b502a7a] editing mrmr\n",
      " 1 file changed, 66 insertions(+), 22 deletions(-)\n",
      "Enumerating objects: 9, done.\n",
      "Counting objects: 100% (9/9), done.\n",
      "Delta compression using up to 20 threads\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects: 100% (5/5), 1.01 KiB | 515.00 KiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/GavinLynch04/FastRelief.git\n",
      "   eaccd18..b502a7a  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"editing mrmr\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85286af1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
