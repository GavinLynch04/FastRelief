{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc38dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import chi2 as chi2_sklearn\n",
    "from chi2 import chi2_numba\n",
    "\n",
    "# Set up the test parameters\n",
    "N_SAMPLES = 2000\n",
    "N_FEATURES = 2000\n",
    "N_CLASSES = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Chi-Squared Implementation Benchmark\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Dataset shape: Samples={N_SAMPLES}, Features={N_FEATURES}, Classes={N_CLASSES}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 1. Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=N_SAMPLES,\n",
    "    n_features=N_FEATURES,\n",
    "    n_informative=500,\n",
    "    n_redundant=500,\n",
    "    n_classes=N_CLASSES,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "# The Chi-squared test requires non-negative features (e.g., counts)\n",
    "X = np.abs(X * 100).astype(np.int64)\n",
    "\n",
    "# 2. Run the Numba implementation\n",
    "# First run is for JIT compilation (\"warm-up\") and is not timed.\n",
    "print(\"Compiling Numba function...\")\n",
    "chi2_numba(X, y)\n",
    "print(\"Compilation complete.\\n\")\n",
    "\n",
    "# Time the Numba implementation\n",
    "print(\"Timing Numba implementation...\")\n",
    "numba_time = timeit.timeit(lambda: chi2_numba(X, y), number=10)\n",
    "print(f\"Done.\")\n",
    "\n",
    "# 3. Run the scikit-learn implementation\n",
    "print(\"\\nTiming scikit-learn implementation...\")\n",
    "sklearn_time = timeit.timeit(lambda: chi2_sklearn(X, y), number=10)\n",
    "print(f\"Done.\")\n",
    "\n",
    "# 4. Verify that the results are the same\n",
    "chi2_n, p_n = chi2_numba(X, y)\n",
    "chi2_s, p_s = chi2_sklearn(X, y)\n",
    "\n",
    "assert np.allclose(chi2_n, chi2_s), \"Chi2 statistics do not match!\"\n",
    "assert np.allclose(p_n, p_s), \"P-values do not match!\"\n",
    "print(\"\\nCorrectness check passed: Results are identical.\")\n",
    "\n",
    "# 5. Report the results\n",
    "print(\"\\n\\n--- Benchmark Results ---\")\n",
    "print(f\"Scikit-learn time: {sklearn_time:.4f} seconds\")\n",
    "print(f\"Numba time:        {numba_time:.4f} seconds\")\n",
    "\n",
    "speedup = sklearn_time / numba_time\n",
    "print(f\"\\nNumba implementation is {speedup:.2f}x faster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37ec6bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main f8861a9] editing documentation\n",
      " 2 files changed, 60 insertions(+), 24 deletions(-)\n",
      "Enumerating objects: 13, done.\n",
      "Counting objects: 100% (13/13), done.\n",
      "Delta compression using up to 20 threads\n",
      "Compressing objects: 100% (7/7), done.\n",
      "Writing objects: 100% (7/7), 1.40 KiB | 1.40 MiB/s, done.\n",
      "Total 7 (delta 5), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
      "To https://github.com/GavinLynch04/FastSelect.git\n",
      "   bdf68e1..f8861a9  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git commit -a -m \"editing documentation\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb3a8b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\r\n",
      "Your branch is up to date with 'origin/main'.\r\n",
      "\r\n",
      "Changes not staged for commit:\r\n",
      "  (use \"git add <file>...\" to update what will be committed)\r\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\r\n",
      "\t\u001b[31mmodified:   BenchmarkingChi2.ipynb\u001b[m\r\n",
      "\r\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b96260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data: 1600 samples, 30000 features, 3 states\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:38<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmarking: My mRMR (Numba) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.517378568649292\n",
      "0.779015302658081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process MemTimer-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/galynch/.local/lib/python3.10/site-packages/memory_profiler.py\", line 262, in run\n",
      "    stop = self.pipe.poll(self.interval)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 424, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# benchmark_mrmr.py\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "from src.fast_select.mRMR import mRMR as My_mRMR\n",
    "\n",
    "from mrmr import mrmr_classif\n",
    "\n",
    "def generate_data(n_samples, n_features, n_states):\n",
    "    \"\"\"Creates a random discrete integer dataset.\"\"\"\n",
    "    print(f\"Generating data: {n_samples} samples, {n_features} features, {n_states} states\")\n",
    "    X = np.random.randint(0, n_states, size=(n_samples, n_features), dtype=np.int8)\n",
    "    y = np.random.randint(0, 2, size=n_samples, dtype=np.int8)\n",
    "    return X, y\n",
    "\n",
    "def benchmark_estimator(name, estimator, X, y):\n",
    "    \"\"\"Measures runtime and peak memory for an estimator's fit method.\"\"\"\n",
    "    print(f\"\\n--- Benchmarking: {name} ---\")\n",
    "\n",
    "    fit_func = lambda: estimator.fit(X, y)\n",
    "\n",
    "    mem_samples = memory_usage(fit_func, interval=0.01)\n",
    "    peak_mem = max(mem_samples)\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    fit_func()\n",
    "    end_time = time.perf_counter()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    print(f\"Runtime:         {runtime:.4f} seconds\")\n",
    "    print(f\"Peak Memory:     {peak_mem:.2f} MiB\")\n",
    "    print(f\"Selected Indicies: {estimator.top_features_}\")\n",
    "    return runtime, peak_mem\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    N_SAMPLES = 1600\n",
    "    N_FEATURES = 30000\n",
    "    N_STATES = 3  # Number of unique values per feature\n",
    "    K_FEATURES_TO_SELECT = 100\n",
    "\n",
    "    X_data, y_data = generate_data(N_SAMPLES, N_FEATURES, N_STATES)\n",
    "    feature_names = [f'feature_{i}' for i in range(N_FEATURES)]\n",
    "\n",
    "    X_pandas = pd.DataFrame(X_data, columns=feature_names)\n",
    "\n",
    "    y_pandas = pd.Series(y_data, name='target')\n",
    "\n",
    "    # --- Define Estimators ---\n",
    "    estimators = {\n",
    "        \"My mRMR (Numba)\": My_mRMR(\n",
    "            n_features_to_select=K_FEATURES_TO_SELECT,\n",
    "            method='MID',\n",
    "            backend='gpu'\n",
    "        ),\n",
    "        \"mrmr_selection\": mrmr_classif(\n",
    "            X=X_pandas, y=y_pandas, K=K_FEATURES_TO_SELECT\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    benchmark_estimator(\"My mRMR (Numba)\", estimators[\"My mRMR (Numba)\"], X_data, y_data)\n",
    "    \n",
    "    print(\"\\n--- Benchmarking: mrmr_selection ---\")\n",
    "    \n",
    "    target_func = lambda: mrmr_classif(X=X_pandas, y=y_pandas, K=K_FEATURES_TO_SELECT)\n",
    "    \n",
    "    mem_samples = memory_usage(target_func, interval=0.01)\n",
    "    peak_mem = max(mem_samples)\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    selected_features = target_func()\n",
    "    end_time = time.perf_counter()\n",
    "    runtime = end_time - start_time\n",
    "    \n",
    "    print(f\"Runtime:         {runtime:.4f} seconds\")\n",
    "    print(f\"Peak Memory:     {peak_mem:.2f} MiB\")\n",
    "    print(f\"Selected features: {selected_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fcddb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/galynch/snap/snapd-desktop-integration/253/Desktop/FastSelect\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b1a1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a synthetic dataset...\n",
      "Discretizing data...\n",
      "--------------------------------------------------\n",
      "Benchmark running on data with shape: (100, 500)\n",
      "--------------------------------------------------\n",
      "Benchmarking YOUR Numba-based CFS implementation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galynch/.local/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/galynch/.local/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 22.1363 seconds\n",
      "Peak memory usage: 36.14 MiB\n",
      "Number of features selected: 16\n",
      "Selected indices: [ 11  53  54  61  73  93 110 111 131 139 148 159 188 215 295 490]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tracemalloc  # <-- Import the memory tracking module\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Assume your CFS class and its helper functions are in a file named `my_cfs.py`\n",
    "from src.fast_select.CFS import CFS\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Helper function for memory profiling\n",
    "# =============================================================================\n",
    "def profile_memory(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Profiles the peak memory usage of a function call.\n",
    "\n",
    "    Returns:\n",
    "        (any, float): A tuple containing the function's return value\n",
    "                      and the peak memory used in MiB.\n",
    "    \"\"\"\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    # Run the function and get its result\n",
    "    result = func(*args, **kwargs)\n",
    "    \n",
    "    # Get memory usage statistics\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    \n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    # Convert peak memory from bytes to Mebibytes (MiB) for readability\n",
    "    peak_mem_mib = peak / 1024**2\n",
    "    \n",
    "    return result, peak_mem_mib\n",
    "\n",
    "# =============================================================================\n",
    "# Main benchmark\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Setup a challenging dataset ---\n",
    "print(\"Generating a synthetic dataset...\")\n",
    "n_samples = 100\n",
    "n_features = 500\n",
    "n_informative = 15\n",
    "n_redundant = 50\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_informative,\n",
    "    n_redundant=n_redundant,\n",
    "    n_classes=3,\n",
    "    flip_y=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Discretizing data...\")\n",
    "discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "X_discrete = discretizer.fit_transform(X).astype(np.int8)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Benchmark running on data with shape: {X_discrete.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 2. Benchmark your implementation ---\n",
    "print(\"Benchmarking YOUR Numba-based CFS implementation...\")\n",
    "my_cfs_selector = CFS(n_jobs=-1)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "# Use the memory profiler to run the fit method\n",
    "_, my_cfs_mem_peak = profile_memory(my_cfs_selector.fit, X_discrete, y)\n",
    "duration_my_cfs = time.perf_counter() - start_time\n",
    "\n",
    "my_cfs_indices = my_cfs_selector.selected_indices_\n",
    "print(f\"Time taken: {duration_my_cfs:.4f} seconds\")\n",
    "print(f\"Peak memory usage: {my_cfs_mem_peak:.2f} MiB\") # <-- New output\n",
    "print(f\"Number of features selected: {len(my_cfs_indices)}\")\n",
    "print(f\"Selected indices: {my_cfs_indices}\")\n",
    "print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5811a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
