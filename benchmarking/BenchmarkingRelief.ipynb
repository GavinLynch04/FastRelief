{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5fcbaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# --- Your other imports ---\n",
    "from skrebate import SURF, ReliefF\n",
    "from skrebate import MultiSURF as SkrebateMultiSURF\n",
    "\n",
    "from src.fast_select.MultiSURF import MultiSURF as FastMultiSURF\n",
    "from src.fast_select.ReliefF import ReliefF as FastReliefF\n",
    "from src.fast_select.SURF import SURF as FastSURF\n",
    "\n",
    "# --- GPU Detection ---\n",
    "try:\n",
    "    from numba import cuda\n",
    "    GPU_AVAILABLE = cuda.is_available()\n",
    "except (ImportError, cuda.cudadrv.error.CudaSupportError):\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "# --- Benchmark Configuration ---\n",
    "P_DOMINANT_SCENARIOS = {\n",
    "    \"name\": \"p >> n (Features Dominant)\",\n",
    "    \"fixed_param\": \"n_samples\", \"fixed_value\": 500,\n",
    "    \"varied_param\": \"n_features\", \"varied_range\": [100, 200, 300, 400, 500]\n",
    "}\n",
    "N_DOMINANT_SCENARIOS = {\n",
    "    \"name\": \"n >> p (Samples Dominant)\",\n",
    "    \"fixed_param\": \"n_features\", \"fixed_value\": 100,\n",
    "    \"varied_param\": \"n_samples\", \"varied_range\": [500, 1000, 1500, 2000, 2500]\n",
    "}\n",
    "N_FEATURES_TO_SELECT = 10\n",
    "N_REPEATS = 3 # Increase repeats for more stable results\n",
    "\n",
    "# --- Estimators to Test ---\n",
    "estimators = {\n",
    "    # skrebate estimators\n",
    "    \"skrebate.ReliefF\": ReliefF(n_features_to_select=N_FEATURES_TO_SELECT,\n",
    "                                n_neighbors=10, n_jobs=-1),\n",
    "    \"skrebate.SURF\": SURF(n_features_to_select=N_FEATURES_TO_SELECT, n_jobs=-1),\n",
    "    \"skrebate.MultiSURF\": SkrebateMultiSURF(n_features_to_select=N_FEATURES_TO_SELECT,\n",
    "                                            n_jobs=-1),\n",
    "    # fast-select CPU estimators\n",
    "    \"fast_select.ReliefF (CPU)\": FastReliefF(n_features_to_select=N_FEATURES_TO_SELECT,\n",
    "                                             n_neighbors=10, backend='cpu', n_jobs=-1),\n",
    "    \"fast_select.SURF (CPU)\": FastSURF(n_features_to_select=N_FEATURES_TO_SELECT,\n",
    "                                       n_jobs=-1),\n",
    "    \"fast_select.MultiSURF (CPU)\": FastMultiSURF(n_features_to_select=N_FEATURES_TO_SELECT,\n",
    "                                                 backend='cpu', n_jobs=-1),\n",
    "}\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"NVIDIA GPU detected. Including GPU benchmarks.\")\n",
    "    estimators.update({\n",
    "        \"fast_select.ReliefF (GPU)\": FastReliefF(n_features_to_select=N_FEATURES_TO_SELECT,\n",
    "                                                 n_neighbors=10, backend='gpu'),\n",
    "        \"fast_select.SURF (GPU)\": FastSURF(n_features_to_select=N_FEATURES_TO_SELECT,\n",
    "                                           backend='gpu'),\n",
    "        \"fast_select.MultiSURF (GPU)\": FastMultiSURF(n_features_to_select=N_FEATURES_TO_SELECT,\n",
    "                                                     backend='gpu'),\n",
    "    })\n",
    "else:\n",
    "    print(\"No NVIDIA GPU detected. Skipping GPU benchmarks.\")\n",
    "\n",
    "# --- CORRECTED BENCHMARK FUNCTION ---\n",
    "def run_single_benchmark(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Measures runtime and peak memory usage of a single estimator fit.\n",
    "    This version performs only ONE execution and correctly measures GPU memory.\n",
    "    \"\"\"\n",
    "    is_gpu_estimator = hasattr(estimator, 'backend') and estimator.backend == 'gpu'\n",
    "\n",
    "    # Use a lambda to wrap the fit call\n",
    "    fit_func = lambda: estimator.fit(X, y)\n",
    "\n",
    "    # --- Memory Measurement ---\n",
    "    peak_mem_mb = 0\n",
    "    if is_gpu_estimator:\n",
    "        # For GPU, we measure VRAM usage directly.\n",
    "        # This requires the fit function to be run inside the context.\n",
    "        ctx = cuda.current_context()\n",
    "        start_mem = ctx.get_memory_info().free\n",
    "        fit_func() # Run the function\n",
    "        end_mem = ctx.get_memory_info().free\n",
    "        # Peak memory is the reduction in free memory.\n",
    "        peak_mem_mb = (start_mem - end_mem) / (1024**2)\n",
    "    else:\n",
    "        # For CPU, memory_profiler works perfectly.\n",
    "        mem_samples = memory_usage(fit_func, interval=0.1)\n",
    "        peak_mem_mb = max(mem_samples)\n",
    "\n",
    "    # --- Runtime Measurement ---\n",
    "    # Since the function has already been run once for memory profiling,\n",
    "    # we time a second run to get a pure execution time without JIT overhead.\n",
    "    # This is now a consistent measurement.\n",
    "    start_time = time.perf_counter()\n",
    "    fit_func()\n",
    "    end_time = time.perf_counter()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    return runtime, peak_mem_mb\n",
    "\n",
    "def warmup_jit_compilers(estimators_dict):\n",
    "    \"\"\"Performs a 'warm-up' run on a small dataset to compile JIT functions.\"\"\"\n",
    "    print(\"\\n--- Warming up JIT compilers ---\")\n",
    "    X_warmup, y_warmup = make_classification(n_samples=20, n_features=20, random_state=42)\n",
    "    for name, estimator in estimators_dict.items():\n",
    "        # More robust check for our custom estimators\n",
    "        if \"fast_select\" in name:\n",
    "            print(f\"  Warming up {name}...\")\n",
    "            try:\n",
    "                # Use a fresh clone for warmup\n",
    "                clone(estimator).fit(X_warmup, y_warmup)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"  > Warm-up FAILED for {name}. Reason: {type(e).__name__}: {e}\")\n",
    "    print(\"--- Warm-up complete ---\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run all benchmark scenarios.\"\"\"\n",
    "    results = []\n",
    "    warmup_jit_compilers(estimators)\n",
    "\n",
    "    scenarios = [P_DOMINANT_SCENARIOS, N_DOMINANT_SCENARIOS]\n",
    "\n",
    "    for scenario_params in scenarios:\n",
    "        scenario_name = scenario_params[\"name\"]\n",
    "        print(f\"\\n--- Running Scenario: {scenario_name} ---\")\n",
    "\n",
    "        fixed_param = scenario_params[\"fixed_param\"]\n",
    "        varied_param = scenario_params[\"varied_param\"]\n",
    "\n",
    "        for varied_value in scenario_params[\"varied_range\"]:\n",
    "            # Set up dataset dimensions for this run\n",
    "            if fixed_param == \"n_samples\":\n",
    "                n_samples = scenario_params[\"fixed_value\"]\n",
    "                n_features = varied_value\n",
    "            else:\n",
    "                n_samples = varied_value\n",
    "                n_features = scenario_params[\"fixed_value\"]\n",
    "\n",
    "            print(f\"\\nGenerating data: {n_samples} samples, {n_features} features\")\n",
    "            X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=20,\n",
    "                                       n_redundant=50, random_state=42)\n",
    "\n",
    "            for name, base_estimator in estimators.items():\n",
    "                for i in range(N_REPEATS):\n",
    "                    print(f\"  Benchmarking {name} (Run {i+1}/{N_REPEATS})...\")\n",
    "                    try:\n",
    "                        estimator = clone(base_estimator)\n",
    "                        runtime, peak_mem = run_single_benchmark(estimator, X, y)\n",
    "                        results.append({\n",
    "                            \"scenario\": scenario_name, \"algorithm\": name,\n",
    "                            \"n_samples\": n_samples, \"n_features\": n_features,\n",
    "                            \"runtime_sec\": runtime, \"peak_memory_mb\": peak_mem\n",
    "                        })\n",
    "                    except Exception:\n",
    "                        warnings.warn(f\"  > FAILED: {name} on {n_samples}x{n_features}.\",\n",
    "                                      \"Reason: {type(e).__name__}: {e}\", UserWarning)\n",
    "\n",
    "    # --- Save results to CSV ---\n",
    "    df = pd.DataFrame(results)\n",
    "    output_file = \"benchmark_results_with_memory.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nBenchmarking complete. Results saved to '{output_file}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05174a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 9d97969] version 0.1.3\r\n",
      " 1 file changed, 42 insertions(+), 21 deletions(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git commit -a -m \"version 0.1.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bac990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mdevelopment\u001b[m\r\n",
      "  main\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ed576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_benchmarks.py\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_scenario(df, scenario_name, x_axis, y_axis, y_label,\n",
    "                  title, filename, use_log_scale=True):\n",
    "    \"\"\"\n",
    "    Generic helper function to generate and save a plot for a given scenario.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The full results dataframe.\n",
    "        scenario_name (str): The name of the scenario to filter for (e.g., 'p >> n').\n",
    "        x_axis (str): The column name for the x-axis (e.g., 'n_features').\n",
    "        y_axis (str): The column name for the y-axis (e.g., 'runtime_sec').\n",
    "        y_label (str): The descriptive label for the y-axis.\n",
    "        title (str): The main title for the plot.\n",
    "        filename (str): The output filename for the saved plot.\n",
    "        use_log_scale (bool): Whether to use a logarithmic scale for the y-axis.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame for the specific scenario\n",
    "    scenario_df = df[df['scenario'] == scenario_name].copy()\n",
    "\n",
    "    # Create a new figure and axes for the plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Use seaborn for a clean, publication-quality line plot\n",
    "    sns.lineplot(\n",
    "        data=scenario_df,\n",
    "        x=x_axis,\n",
    "        y=y_axis,\n",
    "        hue='algorithm',\n",
    "        marker='o',\n",
    "        linewidth=2.5,\n",
    "        errorbar='sd' # 'ci' is deprecated; 'errorbar' is the new standard\n",
    "    )\n",
    "\n",
    "    # Set plot properties\n",
    "    plt.title(title, fontsize=18, fontweight='bold', pad=20)\n",
    "    plt.xlabel(x_axis.replace('_', ' ').title(), fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "\n",
    "    if use_log_scale:\n",
    "        plt.yscale('log')\n",
    "\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", c='0.7')\n",
    "    plt.legend(title='Algorithm', fontsize=11, title_fontsize=13)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved to '{filename}'\")\n",
    "    plt.close() # Close the figure to free up memory\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load results and generate all plots.\"\"\"\n",
    "    input_file = \"benchmark_results_with_memory.csv\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: '{input_file}' not found.\", file=sys.stderr)\n",
    "        print(\"Please run the updated benchmark script first.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Validate that the necessary columns exist\n",
    "    required_cols = ['scenario', 'algorithm', 'n_samples', 'n_features',\n",
    "                     'runtime_sec', 'peak_memory_mb']\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: The CSV file '{input_file}' is missing required columns.\",\n",
    "              file=sys.stderr)\n",
    "        print(f\"Expected columns: {required_cols}\", file=sys.stderr)\n",
    "        print(f\"Found columns: {list(df.columns)}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Use a professional plot style\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    # --- Runtime Plots ---\n",
    "    print(\"\\n--- Generating Runtime Plots ---\")\n",
    "    plot_scenario(\n",
    "        df=df,\n",
    "        scenario_name='p >> n (Features Dominant)',\n",
    "        x_axis='n_features',\n",
    "        y_axis='runtime_sec',\n",
    "        y_label=\"Runtime (seconds, log scale)\",\n",
    "        title='Benchmark: Runtime vs. Number of Features (p >> n)\\n(n_samples fixed)',\n",
    "        filename='benchmark_p_dominant_runtime.png',\n",
    "        use_log_scale=True\n",
    "    )\n",
    "    plot_scenario(\n",
    "        df=df,\n",
    "        scenario_name='n >> p (Samples Dominant)',\n",
    "        x_axis='n_samples',\n",
    "        y_axis='runtime_sec',\n",
    "        y_label=\"Runtime (seconds, log scale)\",\n",
    "        title='Benchmark: Runtime vs. Number of Samples (n >> p)\\n(n_features fixed)',\n",
    "        filename='benchmark_n_dominant_runtime.png',\n",
    "        use_log_scale=True\n",
    "    )\n",
    "\n",
    "    # --- Memory Usage Plots ---\n",
    "    print(\"\\n--- Generating Memory Usage Plots ---\")\n",
    "    plot_scenario(\n",
    "        df=df,\n",
    "        scenario_name='p >> n (Features Dominant)',\n",
    "        x_axis='n_features',\n",
    "        y_axis='peak_memory_mb',\n",
    "        y_label=\"Peak Memory Usage (MB, log scale)\",\n",
    "        title='Benchmark: Memory vs. Number of Features (p >> n)\\n(n_samples fixed)',\n",
    "        filename='benchmark_p_dominant_memory.png',\n",
    "        use_log_scale=True # Memory can also vary greatly, log scale is often useful\n",
    "    )\n",
    "    plot_scenario(\n",
    "        df=df,\n",
    "        scenario_name='n >> p (Samples Dominant)',\n",
    "        x_axis='n_samples',\n",
    "        y_axis='peak_memory_mb',\n",
    "        y_label=\"Peak Memory Usage (MB, log scale)\",\n",
    "        title='Benchmark: Memory vs. Number of Samples (n >> p)\\n(n_features fixed)',\n",
    "        filename='benchmark_n_dominant_memory.png',\n",
    "        use_log_scale=True\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4adc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
