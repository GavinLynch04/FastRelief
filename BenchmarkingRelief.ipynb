{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5fcbaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galynch/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GPU detected. Including GPU benchmarks.\n",
      "\n",
      "--- Warming up JIT compilers ---\n",
      "  Warming up fast_relief.ReliefF (CPU)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galynch/.local/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warming up fast_relief.MultiSURF (CPU)...\n",
      "  Warming up fast_relief.ReliefF (GPU)...\n",
      "  Warming up fast_relief.MultiSURF (GPU)...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import make_classification\n",
    "import threading\n",
    "import os\n",
    "\n",
    "# --- Plotting Libraries ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Profiling and Estimator Libraries ---\n",
    "from memory_profiler import memory_usage\n",
    "try:\n",
    "    from pynvml import *\n",
    "    pynvml_available = True\n",
    "except ImportError:\n",
    "    pynvml_available = False\n",
    "\n",
    "from skrebate import ReliefF, SURF, SURFstar, MultiSURF as SkrebateMultiSURF, MultiSURFstar\n",
    "# Assuming your local implementations are in a 'src' directory\n",
    "from src.fast_relief.ReliefF import ReliefF as FastReliefF\n",
    "from src.fast_relief.SURF import SURF as FastSURF\n",
    "from src.fast_relief.MultiSURF import MultiSURF as FastMultiSURF\n",
    "\n",
    "try:\n",
    "    from numba import cuda\n",
    "    GPU_AVAILABLE = cuda.is_available()\n",
    "    if pynvml_available and GPU_AVAILABLE:\n",
    "        nvmlInit()\n",
    "except (ImportError, NVMLError):\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "# --- Benchmark Configuration ---\n",
    "P_DOMINANT_SCENARIOS = { \"n_samples\": 100, \"n_features_range\": [100000, 200000, 300000, 400000, 500000] }\n",
    "N_DOMINANT_SCENARIOS = { \"n_features\": 100, \"n_samples_range\": [10000, 20000, 30000, 40000, 50000] }\n",
    "N_FEATURES_TO_SELECT = 10\n",
    "N_REPEATS = 1 # Increased repeats for more stable averages in plots\n",
    "\n",
    "# --- Estimators to Test ---\n",
    "estimators = {\n",
    "    #\"skrebate.ReliefF\": ReliefF(n_features_to_select=N_FEATURES_TO_SELECT, n_jobs=-1),\n",
    "    #\"skrebate.MultiSURF\": SkrebateMultiSURF(n_features_to_select=N_FEATURES_TO_SELECT, n_jobs=-1),\n",
    "    \"fast_relief.ReliefF (CPU)\": FastReliefF(n_features_to_select=N_FEATURES_TO_SELECT, backend='cpu'),\n",
    "    \"fast_relief.MultiSURF (CPU)\": FastMultiSURF(n_features_to_select=N_FEATURES_TO_SELECT, backend='cpu'),\n",
    "}\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"NVIDIA GPU detected. Including GPU benchmarks.\")\n",
    "    estimators.update({\n",
    "        \"fast_relief.ReliefF (GPU)\": FastReliefF(n_features_to_select=N_FEATURES_TO_SELECT, backend='gpu'),\n",
    "        \"fast_relief.MultiSURF (GPU)\": FastMultiSURF(n_features_to_select=N_FEATURES_TO_SELECT, backend='gpu'),\n",
    "    })\n",
    "else:\n",
    "    print(\"No NVIDIA GPU detected. Skipping GPU benchmarks.\")\n",
    "\n",
    "\n",
    "# --- CORE BENCHMARKING FUNCTIONS (with memory profiling) ---\n",
    "def run_single_benchmark(estimator, X, y, is_gpu=False):\n",
    "    mem_increase_mb = -1.0\n",
    "    def fit_estimator():\n",
    "        estimator.fit(X, y)\n",
    "\n",
    "    if is_gpu and GPU_AVAILABLE and pynvml_available:\n",
    "        handle = nvmlDeviceGetHandleByIndex(0)\n",
    "        class MemTracker(threading.Thread):\n",
    "            def __init__(self):\n",
    "                threading.Thread.__init__(self)\n",
    "                self.peak_mem = 0\n",
    "                self.running = True\n",
    "            def run(self):\n",
    "                initial_mem = nvmlDeviceGetMemoryInfo(handle).used\n",
    "                while self.running:\n",
    "                    self.peak_mem = max(self.peak_mem, nvmlDeviceGetMemoryInfo(handle).used - initial_mem)\n",
    "                    time.sleep(0.01)\n",
    "            def stop(self):\n",
    "                self.running = False\n",
    "        tracker = MemTracker()\n",
    "        tracker.start()\n",
    "        start_time = time.perf_counter()\n",
    "        try:\n",
    "            fit_estimator()\n",
    "        finally:\n",
    "            tracker.stop()\n",
    "            tracker.join()\n",
    "        end_time = time.perf_counter()\n",
    "        mem_increase_mb = tracker.peak_mem / (1024**2)\n",
    "    else:\n",
    "        start_time = time.perf_counter()\n",
    "        mem_profile, _ = memory_usage((fit_estimator,), retval=True, interval=0.1)\n",
    "        end_time = time.perf_counter()\n",
    "        mem_increase_mb = max(mem_profile) - mem_profile[0]\n",
    "        \n",
    "    runtime = end_time - start_time\n",
    "    return runtime, mem_increase_mb\n",
    "\n",
    "def warmup_jit_compilers(estimators_dict):\n",
    "    print(\"\\n--- Warming up JIT compilers ---\")\n",
    "    X_warmup, y_warmup = make_classification(n_samples=10, n_features=10, random_state=42)\n",
    "    for name, estimator in estimators_dict.items():\n",
    "        if \"fast_relief\" in name:\n",
    "            print(f\"  Warming up {name}...\")\n",
    "            try:\n",
    "                clone(estimator).fit(X_warmup, y_warmup)\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"  > Warm-up FAILED for {name}. Reason: {e}\")\n",
    "    print(\"--- Warm-up complete ---\")\n",
    "\n",
    "\n",
    "def plot_scenario(df, scenario_name, x_axis, y_axis, title, filename):\n",
    "    \"\"\"\n",
    "    Generates and saves a line plot for a given benchmark scenario.\n",
    "    Handles potential pandas/matplotlib version conflicts automatically.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    scenario_df = df[df['scenario'] == scenario_name]\n",
    "\n",
    "    # Use seaborn for a clean, publication-quality line plot\n",
    "    # It automatically groups by 'algorithm' and calculates mean/confidence intervals\n",
    "    sns.lineplot(\n",
    "        data=scenario_df,\n",
    "        x=x_axis,\n",
    "        y=y_axis,\n",
    "        hue='algorithm',\n",
    "        marker='o',\n",
    "        linestyle='-',\n",
    "        errorbar=('ci', 95) # Show 95% confidence interval\n",
    "    )\n",
    "\n",
    "    # Adding plot labels and title\n",
    "    plt.title(title, fontsize=18, fontweight='bold')\n",
    "    plt.xlabel(x_axis.replace('_', ' ').title(), fontsize=14)\n",
    "    plt.ylabel(y_axis.replace('_', ' ').title(), fontsize=14)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend(title='Algorithm', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for legend\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Plot saved to '{filename}'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION BLOCK ---\n",
    "def main():\n",
    "    \"\"\"Main function to run all benchmark scenarios and generate plots.\"\"\"\n",
    "    results = []\n",
    "    output_dir = \"benchmark_plots\"\n",
    "    os.makedirs(output_dir, exist_ok=True) # Create directory for plots\n",
    "\n",
    "    warmup_jit_compilers(estimators)\n",
    "\n",
    "    # --- Run Benchmark Scenarios ---\n",
    "    scenarios = {\n",
    "        \"p >> n\": (P_DOMINANT_SCENARIOS, \"n_features\"),\n",
    "        \"n >> p\": (N_DOMINANT_SCENARIOS, \"n_samples\")\n",
    "    }\n",
    "\n",
    "    for name, (params, independent_var) in scenarios.items():\n",
    "        print(f\"\\n--- Running Scenario: {name} ---\")\n",
    "        if name == \"p >> n\":\n",
    "            param_range = params[\"n_features_range\"]\n",
    "            n_samples = params[\"n_samples\"]\n",
    "            for n_features in param_range:\n",
    "                print(f\"\\nGenerating data: {n_samples} samples, {n_features} features\")\n",
    "                X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=20, n_redundant=100, random_state=42)\n",
    "                for est_name, estimator in estimators.items():\n",
    "                    for i in range(N_REPEATS):\n",
    "                        print(f\"  Benchmarking {est_name} (Run {i+1}/{N_REPEATS})...\")\n",
    "                        try:\n",
    "                            is_gpu = \"(GPU)\" in est_name\n",
    "                            runtime, memory_mb = run_single_benchmark(clone(estimator), X, y, is_gpu)\n",
    "                            results.append({\"scenario\": name, \"algorithm\": est_name, \"n_samples\": n_samples, \"n_features\": n_features, \"runtime\": runtime, \"memory_increase_mb\": memory_mb})\n",
    "                        except Exception as e:\n",
    "                            warnings.warn(f\"  > FAILED: {est_name}. Reason: {e}\")\n",
    "        else: # n >> p\n",
    "            param_range = params[\"n_samples_range\"]\n",
    "            n_features = params[\"n_features\"]\n",
    "            for n_samples in param_range:\n",
    "                print(f\"\\nGenerating data: {n_samples} samples, {n_features} features\")\n",
    "                X, y = make_classification(n_samples=n_samples, n_features=n_features, n_informative=20, n_redundant=50, random_state=42)\n",
    "                for est_name, estimator in estimators.items():\n",
    "                    for i in range(N_REPEATS):\n",
    "                        print(f\"  Benchmarking {est_name} (Run {i+1}/{N_REPEATS})...\")\n",
    "                        try:\n",
    "                            is_gpu = \"(GPU)\" in est_name\n",
    "                            runtime, memory_mb = run_single_benchmark(clone(estimator), X, y, is_gpu)\n",
    "                            results.append({\"scenario\": name, \"algorithm\": est_name, \"n_samples\": n_samples, \"n_features\": n_features, \"runtime\": runtime, \"memory_increase_mb\": memory_mb})\n",
    "                        except Exception as e:\n",
    "                            warnings.warn(f\"  > FAILED: {est_name}. Reason: {e}\")\n",
    "    \n",
    "    # --- Save results to CSV ---\n",
    "    df = pd.DataFrame(results)\n",
    "    output_file = \"benchmark_results_with_memory.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nBenchmarking complete. Results saved to '{output_file}'\")\n",
    "\n",
    "    # --- Generate and Save Plots ---\n",
    "    print(\"\\n--- Generating Plots ---\")\n",
    "    \n",
    "    # Plot 1: Runtime for p >> n\n",
    "    plot_scenario(df, 'p >> n', 'n_features', 'runtime', \n",
    "                  'Runtime Performance (Many Features, p >> n)', \n",
    "                  os.path.join(output_dir, 'p_dominant_runtime.png'))\n",
    "\n",
    "    # Plot 2: Memory for p >> n\n",
    "    plot_scenario(df, 'p >> n', 'n_features', 'memory_increase_mb', \n",
    "                  'Memory Usage (Many Features, p >> n)', \n",
    "                  os.path.join(output_dir, 'p_dominant_memory.png'))\n",
    "\n",
    "    # Plot 3: Runtime for n >> p\n",
    "    plot_scenario(df, 'n >> p', 'n_samples', 'runtime', \n",
    "                  'Runtime Performance (Many Samples, n >> p)', \n",
    "                  os.path.join(output_dir, 'n_dominant_runtime.png'))\n",
    "\n",
    "    # Plot 4: Memory for n >> p\n",
    "    plot_scenario(df, 'n >> p', 'n_samples', 'memory_increase_mb', \n",
    "                  'Memory Usage (Many Samples, n >> p)', \n",
    "                  os.path.join(output_dir, 'n_dominant_memory.png'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Best practice is to have your script execute via a main function call\n",
    "    main()\n",
    "    \n",
    "    # Clean up NVML\n",
    "    if GPU_AVAILABLE and pynvml_available:\n",
    "        try:\n",
    "            nvmlShutdown()\n",
    "        except NVMLError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05174a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 3d0415a] editing structure\n",
      " 3 files changed, 18 insertions(+), 27 deletions(-)\n",
      "Enumerating objects: 13, done.\n",
      "Counting objects: 100% (13/13), done.\n",
      "Delta compression using up to 20 threads\n",
      "Compressing objects: 100% (7/7), done.\n",
      "Writing objects: 100% (7/7), 822 bytes | 822.00 KiB/s, done.\n",
      "Total 7 (delta 6), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (6/6), completed with 6 local objects.\u001b[K\n",
      "To https://github.com/GavinLynch04/FastSelect.git\n",
      "   11ae503..3d0415a  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git commit -a -m \"editing structure\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2c79e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: *** No rule to make target 'clean'.  Stop.\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9eaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
